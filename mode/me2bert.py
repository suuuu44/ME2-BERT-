# -*- coding: utf-8 -*-
import torch
import torch.nn as nn
import torch.nn.functional as F

from models.bert_backbone import BertBackbone
from models.e_dae import EmotionDAE
from models.lstm_gate import LSTMGateFusion
from models.domain_classifier import DomainClassifier
from models.moral_classifier import MoralClassifier


class ME2BERT(nn.Module):
    """
    ME2-BERT æ•´ä½“æ¨¡å‹
    ------------------------------------------------------------
    - æ¨¡å—ç»“æ„:
        1) BERT ç¼–ç å™¨
        2) Emotion-aware Denoising Autoencoder (E-DAE)
        3) LSTM é—¨æ§èåˆæ¨¡å—
        4) Moral Classifierï¼ˆå¤šæ ‡ç­¾ï¼‰
        5) Domain Classifierï¼ˆå¸¦ GRL çš„åŸŸå¯¹æŠ—ï¼‰
    ------------------------------------------------------------
    ç›®æ ‡:
        L_total = L_MSE + L_contrast + L_MF + Î» * L_domain
    """

    def __init__(self,
                 num_labels=5,
                 hidden_dim=768,
                 dae_bottleneck_dim=512,
                 lambda_dom=0.1,
                 bert_model_name="bert-base-uncased",
                 device="cpu",
                 use_edae=True):
        super().__init__()
        self.device = device
        self.use_edae = use_edae
        self.lambda_dom = lambda_dom

        # === æ¨¡å—åˆå§‹åŒ– ===
        self.bert_backbone = BertBackbone(model_name=bert_model_name, device=device)

        # E-DAE æ¨¡å—
        self.e_dae = EmotionDAE(
            input_dim=hidden_dim,
            bottleneck_dim=dae_bottleneck_dim,
            device=device
        )

        # é—¨æ§èåˆå±‚
        self.lstm_gate = LSTMGateFusion(hidden_dim=hidden_dim)

        # åŸŸåˆ†ç±»å™¨ï¼ˆæ ¹æ® use_edae å†³å®šè¾“å…¥ç»´åº¦ï¼‰
        self.domain_classifier = DomainClassifier(
            input_dim=dae_bottleneck_dim if self.use_edae else hidden_dim
        )

        # é“å¾·åˆ†ç±»å™¨
        self.moral_classifier = MoralClassifier(
            input_dim=hidden_dim,
            hidden_dim=256,
            num_labels=num_labels
        )

    def forward(self, input_ids, attention_mask,
                labels_moral=None, labels_domain=None, labels_emotion=None,
                contrast_mask=None, contrast_weight=None,
                alpha=1.0, margin=0.2):
        """
        Args:
            input_ids, attention_mask: BERT tokenizer è¾“å‡º
            labels_moral: [B, num_labels]
            labels_domain: [B] æˆ– [B, 1]
            labels_emotion: [B] 0..4
            contrast_mask: [B] 1=å‚ä¸å¯¹æ¯”, 0=è·³è¿‡ (å¦‚ no_emotion)
            contrast_weight: [B] å¯¹æ¯”æ ·æœ¬æƒé‡
            alpha: GRL ç³»æ•° (åŠ¨æ€è°ƒæ•´)
            margin: Triplet å¯¹æ¯”æŸå¤± margin
        Returns:
            total_loss: æ ‡é‡
            outputs: dict
        """
        use_edae = self.use_edae  # âœ… ç»Ÿä¸€æ¥æº

        # === 1ï¸âƒ£ BERT è¡¨ç¤º ===
        x_bert_seq, _ = self.bert_backbone(input_ids, attention_mask)  # [B, T, H]

        # === 2ï¸âƒ£ E-DAE é‡æ„ & å¯¹æ¯”å­¦ä¹  ===
        if labels_domain is not None and labels_domain.dim() == 2:
            labels_domain = labels_domain.squeeze(1)

        if use_edae:
            x_recon, bottleneck_seq, bottleneck_pooled, loss_edae_dict = self.e_dae(
                bert_embeds=x_bert_seq,
                attention_mask=attention_mask,
                emo_labels=labels_emotion,
                domain_labels=labels_domain.long() if labels_domain is not None else None,
                contrast_mask=contrast_mask,
                contrast_weight=contrast_weight,
                margin=margin
            )
            loss_edae = sum(loss_edae_dict.values())  # L_MSE + L_contrast
        else:
            # ğŸ”¹ å¦‚æœä¸ä½¿ç”¨ E-DAEï¼Œåˆ™ä¸è¿›è¡Œé‡æ„ï¼Œloss_edae è®¾ä¸º 0
            x_recon = None
            bottleneck_pooled = x_bert_seq.mean(dim=1)
            loss_edae = torch.tensor(0.0, device=x_bert_seq.device)

        # === 3ï¸âƒ£ é—¨æ§èåˆï¼ˆæ ¹æ® use_edae æ§åˆ¶ï¼‰
        if use_edae:
            # ğŸ”¹ ä½¿ç”¨ E-DAE è¾“å‡ºè¿›è¡Œé—¨æ§èåˆ
            x_fused = self.lstm_gate(x_bert_seq, x_recon)  # [B, T, H]
        else:
            # ğŸ”¹ Baselineï¼šç›´æ¥ä½¿ç”¨ BERT è¡¨ç¤ºï¼ˆä¸èåˆï¼‰
            x_fused = x_bert_seq

        # === 4ï¸âƒ£ é“å¾·åˆ†ç±» (L_MF) ===
        logits_moral, probs_moral = self.moral_classifier(x_fused, attention_mask=attention_mask)
        loss_moral = None
        if labels_moral is not None:
            loss_moral = F.binary_cross_entropy_with_logits(logits_moral, labels_moral.float())

        # === 5ï¸âƒ£ åŸŸåˆ†ç±» (GRL + BCE) ===
        if use_edae:
            dom_input = bottleneck_pooled  # [B,512]
        else:
            dom_input = x_bert_seq.mean(dim=1)  # [B,768]

        logits_domain = self.domain_classifier(dom_input, alpha=alpha)
        loss_adv = None
        if labels_domain is not None:
            loss_adv = F.binary_cross_entropy_with_logits(
                logits_domain.view(-1), labels_domain.float().view(-1)
            )

        # === 6ï¸âƒ£ æ€»æŸå¤± ===
        total_loss = (
                (loss_moral or 0)
                + self.lambda_dom * (loss_adv or 0)
                + (loss_edae or 0)
        )

        return total_loss, {
            "moral_probs": torch.sigmoid(logits_moral).detach(),
            "domain_probs": torch.sigmoid(logits_domain).detach(),
            "loss_edae": float(loss_edae.item()) if torch.is_tensor(loss_edae) else 0.0,
            "loss_moral": float(loss_moral.item()) if loss_moral is not None else 0.0,
            "loss_adv": float(loss_adv.item()) if loss_adv is not None else 0.0,
        }


# === æµ‹è¯• ===
if __name__ == "__main__":
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

    texts = ["This is a source domain example.", "This is a target domain example."]
    encodings = tokenizer(texts, padding=True, truncation=True, max_length=12, return_tensors="pt")

    labels_moral = torch.randint(0, 2, (2, 5)).float()
    labels_domain = torch.randint(0, 2, (2,))
    labels_emotion = torch.randint(0, 5, (2,))
    contrast_mask = (labels_emotion != 4).float()

    model = ME2BERT(num_labels=5, device="cpu")
    total_loss, outputs = model(
        encodings["input_ids"], encodings["attention_mask"],
        labels_moral=labels_moral,
        labels_domain=labels_domain,
        labels_emotion=labels_emotion,
        contrast_mask=contrast_mask,
        alpha=1.0
    )

    print(f"æ€»æŸå¤±: {total_loss.item() if total_loss is not None else None:.4f}")
    print("L_MSE+Contrast:", outputs["loss_edae"])
    print("L_MF:", outputs["loss_moral"])
    print("L_Domain:", outputs["loss_adv"])
